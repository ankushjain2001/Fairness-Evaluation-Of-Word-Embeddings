{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nutritional Labels for ADS: The Effect of Word Embeddings on Bias\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import plotly\n",
    "import colorlover as cl\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "plotly.tools.set_credentials_file(username='nholloway', api_key='Ef8vuHMUdvaIpvtC2lux')\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "GLOVE_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "NUMBERBATCH_PATH = '../input/conceptnet-numberbatch-vectors/numberbatch-en-17.06.txt/numberbatch-en-17.06.txt'\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    words = Input(shape=(MAX_LEN,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    s_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    p_mapping = {\"_\":\" \", \"`\":\" \"}    \n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    \n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([s_mapping[t] if t in s_mapping else t for t in text.split(\" \")])\n",
    "    for p in p_mapping:\n",
    "        text = text.replace(p, p_mapping[p])    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we split our train and test set make sure that the test set retains the identity labels to benchmark subgroup bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test_columns = ['target', 'black', 'white', 'male', 'female', 'homosexual_gay_or_lesbian',\n",
    "                'christian', 'jewish', 'muslim', 'psychiatric_or_mental_illness', \n",
    "                'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat'] \n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train['comment_text'], train[test_columns], test_size=.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = x_train.apply(lambda x: preprocess(x.lower()))\n",
    "y_train['target'] = np.where(y_train['target'] >= 0.5, 1, 0)\n",
    "x_val = x_val.apply(lambda x: preprocess(x.lower()))\n",
    "y_val['target'] = np.where(y_val['target'] >= 0.5, 1, 0)\n",
    "y_aux_train = y_train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].copy()\n",
    "y_train = y_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_val))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_val = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train_tokenized_sequenced.txt', x_train)\n",
    "np.save('x_val_tokenized_sequenced.txt', x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def train_model(embedding_matrix, model_name):\n",
    "    checkpoint_predictions = []\n",
    "    weights = []\n",
    "    \n",
    "    for model_idx in range(NUM_MODELS):\n",
    "        model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "        for global_epoch in range(EPOCHS):\n",
    "            model.fit(\n",
    "                x_train,\n",
    "                [y_train, y_aux_train],\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=1,\n",
    "                verbose=2,\n",
    "                callbacks=[\n",
    "                    LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "                ])\n",
    "            checkpoint_predictions.append(model.predict(x_val, batch_size=2048)[0].flatten())\n",
    "            weights.append(2 ** global_epoch)\n",
    "            model.save(f'{model_name}_model.h5')\n",
    "\n",
    "    predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Benchmarks (as per the dataset paper)\n",
    "\n",
    "- Subgroup AUC: The AUC score for the entire subgroup- a low score hear means the model **fails to distinguish between toxic and non-toxic comments** that mention this identity. \n",
    "- BPSN AUC: Background positive, subgroup negative. A low value here means the model confuses **non-toxic examples that mention the identity with toxic examples that do not**.\n",
    "- BNSP AUC: Background negative, subgroup positive. A low value here means that the model confuses **toxic examples that mention the identity with non-toxic examples that do not**. \n",
    "\n",
    "The final score used in this competition is a combination of these bias metrics, which we will also compute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "IDENTITY_COLUMNS = ['black', 'white', 'male', 'female', 'homosexual_gay_or_lesbian',\n",
    "                   'christian', 'jewish', 'muslim', 'psychiatric_or_mental_illness'] \n",
    "    \n",
    "def compute_bpsn_auc(df, subgroup, model, label):\n",
    "    subgroup_positive_examples = df.loc[(df[subgroup] == 1) & (df[label] == 1)]\n",
    "    non_subgroup_negative_examples = df.loc[df[subgroup] != 1 & (df[label] == 0)]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return auc(examples[label], examples[model])  \n",
    "    \n",
    "def compute_bnsp_auc(df, subgroup, model, label):\n",
    "    subgroup_negative_examples = df.loc[(df[subgroup] == 1) & (df[label] == 0)]\n",
    "    non_subgroup_positive_examples = df.loc[(df[subgroup] != 1) & (df[label] == 1)]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return auc(examples[label], examples[model])\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total/len(series), 1/p)\n",
    "\n",
    "def compute_final_bias(bias_df, overall_auc, power=-5, overall_model_weight=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df['subgroup_auc'], power),\n",
    "        power_mean(bias_df['bpsn_auc'], power),\n",
    "        power_mean(bias_df['bnsp_auc'], power)\n",
    "    ])\n",
    "    return (overall_model_weight * overall_auc) + ((1 - overall_model_weight)* bias_score)\n",
    "    \n",
    "def compute_subgroup_bias_metrics(df, subgroups, model, label):\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        subgroup_df = df.loc[df[subgroup] == 1]\n",
    "        record = {\n",
    "            'subgroup': subgroup, \n",
    "            'subgroup_size': len(subgroup_df)\n",
    "        }\n",
    "        record['subgroup_auc'] = auc(subgroup_df['target'], subgroup_df[model])\n",
    "        record['bpsn_auc'] = compute_bpsn_auc(df, subgroup, model, label)\n",
    "        record['bnsp_auc'] = compute_bnsp_auc(df, subgroup, model, label)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', \n",
    "                                                 ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Pretrained Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "def build_control_model(embedding_size, max_features, num_aux_targets):\n",
    "    words = Input(shape=(MAX_LEN,))\n",
    "    x = Embedding(max_features, embedding_size)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_control_model(embedding_size, max_features, model_name):\n",
    "    checkpoint_predictions = []\n",
    "    weights = []\n",
    "    \n",
    "    for model_idx in range(NUM_MODELS):\n",
    "        model = build_control_model(embedding_size, max_features, y_aux_train.shape[-1])\n",
    "        for global_epoch in range(EPOCHS):\n",
    "            model.fit(\n",
    "                x_train,\n",
    "                [y_train, y_aux_train],\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=1,\n",
    "                verbose=2,\n",
    "                callbacks=[\n",
    "                    LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "                ])\n",
    "            checkpoint_predictions.append(model.predict(x_val, batch_size=2048)[0].flatten())\n",
    "            weights.append(2 ** global_epoch)\n",
    "            model.save(f'{model_name}_model.h5')\n",
    "\n",
    "    predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(tokenizer.word_index.keys())\n",
    "emb_size = 300\n",
    "# This model took 3hr 47min to train compared to \n",
    "# ~3hr 3min for most the pretrained embedding models\n",
    "# AJ_PREDS\n",
    "control_preds = train_control_model(emb_size, max_features, 'no_pretrained_embeddings')\n",
    "#results_df = pd.read_pickle('../input/embedding-bias-benchmark/final_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# results_df = pd.concat([pd.DataFrame(control_preds, columns=['w/o pretrained']).reset_index(drop=True), results_df], axis=1)\n",
    "ctrl_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'w/o pretrained', 'target')\n",
    "overall_auc = auc(results_df['target'], results_df['w/o pretrained'])\n",
    "ctrl_final_bias = compute_final_bias(ctrl_bias_metrics, overall_auc)\n",
    "display(ctrl_bias_metrics)\n",
    "print(f'Final Metric: {ctrl_final_bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "fasttext_matrix = build_matrix(tokenizer.word_index, FASTTEXT_PATH)\n",
    "# AJ_PREDS\n",
    "fast_preds = train_model(fasttext_matrix, 'fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fast_preds', fast_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.concat([pd.DataFrame(fast_preds, columns=['fasttext']).reset_index(drop=True), y_val.reset_index(drop=True)], axis=1).fillna(0)\n",
    "ft_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'fasttext', 'target')\n",
    "overall_auc = auc(results_df['target'], results_df['fasttext'])\n",
    "ft_final_bias = compute_final_bias(ft_bias_metrics, overall_auc)\n",
    "display(ft_bias_metrics)\n",
    "print(f'Final Metric: {ft_final_bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVE \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "glove_matrix = build_matrix(tokenizer.word_index, GLOVE_PATH)\n",
    "# AJ_PREDS\n",
    "glove_preds = train_model(glove_matrix, 'glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('glove_preds', glove_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.concat([pd.DataFrame(glove_preds, columns=['glove']).reset_index(drop=True), results_df], axis=1)\n",
    "gl_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'glove', 'target')\n",
    "overall_auc = auc(results_df['target'], results_df['glove'])\n",
    "gl_final_bias = compute_final_bias(gl_bias_metrics, overall_auc)\n",
    "display(gl_bias_metrics)\n",
    "print(f'Final Metric: {gl_final_bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptnet Numberbatch\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "numb_matrix = build_matrix(tokenizer.word_index, NUMBERBATCH_PATH)\n",
    "# AJ_PREDS\n",
    "numb_preds = train_model(numb_matrix, 'numberbatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# results_df = pd.concat([pd.DataFrame(numb_preds, columns=['numberbatch']).reset_index(drop=True), results_df], axis=1)\n",
    "nb_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'numberbatch', 'target')\n",
    "overall_auc = auc(results_df['target'], results_df['numberbatch'])\n",
    "nb_final_bias = compute_final_bias(nb_bias_metrics, overall_auc)\n",
    "display(nb_bias_metrics)\n",
    "print(f'Final Metric: {nb_final_bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert_embedding in /opt/conda/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: typing==3.6.6 in /opt/conda/lib/python3.6/site-packages (from bert_embedding) (3.6.6)\n",
      "Requirement already satisfied: mxnet==1.4.0 in /opt/conda/lib/python3.6/site-packages (from bert_embedding) (1.4.0)\n",
      "Requirement already satisfied: gluonnlp==0.6.0 in /opt/conda/lib/python3.6/site-packages (from bert_embedding) (0.6.0)\n",
      "Requirement already satisfied: numpy==1.14.6 in /opt/conda/lib/python3.6/site-packages (from bert_embedding) (1.14.6)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.6/site-packages (from mxnet==1.4.0->bert_embedding) (2.21.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from mxnet==1.4.0->bert_embedding) (0.8.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (1.22)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install bert_embedding\n",
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "def get_bert_embed_matrix():\n",
    "    %%time\n",
    "    # Total CPU time (my machine): 1d 4h 7min\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "    embedding_results = bert_embedding(vocab)\n",
    "    bert_embeddings = {}\n",
    "    for emb in embedding_results:\n",
    "        try: \n",
    "            bert_embeddings[emb[0][0]] = emb[1][0]\n",
    "        except:\n",
    "            pass\n",
    "    with open('../input/bert.768.pkl', 'wb') as f:\n",
    "        pickle.dump(bert_embeddings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def scale(x):\n",
    "    mini = 272\n",
    "    maxi = 6047\n",
    "    scale_rng = [10, 55]\n",
    "    return (scale_rng[1] - scale_rng[0])*((x-mini)/(maxi-mini))+scale_rng[0]\n",
    "'''    \n",
    "trace0 = go.Scatter(\n",
    "{\n",
    "        'x': ctrl_bias_metrics['subgroup_auc'], \n",
    "        'y': ctrl_bias_metrics['subgroup_size'],\n",
    "        'legendgroup': 'w/o pretrained',\n",
    "        'name': 'w/o pretrained', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][0],\n",
    "            'size': [scale(x) for x in ctrl_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': ctrl_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "{\n",
    "        'x': ft_bias_metrics['subgroup_auc'], \n",
    "        'y': ft_bias_metrics['subgroup_size'],\n",
    "        'legendgroup': 'fasttext',\n",
    "        'name': 'fasttext', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][2],\n",
    "            'size': [scale(x) for x in ft_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': ft_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "{\n",
    "        'x': gl_bias_metrics['subgroup_auc'], \n",
    "        'y': gl_bias_metrics['subgroup_size'],\n",
    "        'legendgroup': 'glove',\n",
    "        'name': 'glove', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][4],\n",
    "            'size': [scale(x) for x in gl_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': gl_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "{\n",
    "        'x': nb_bias_metrics['subgroup_auc'], \n",
    "        'y': nb_bias_metrics['subgroup_size'],\n",
    "        'legendgroup': 'numberbatch',\n",
    "        'name': 'numberbatch', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][6],\n",
    "            'size': [scale(x) for x in nb_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': nb_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "layout = go.Layout(\n",
    "    title= 'Subgroup Size vs Subgroup AUC',\n",
    "    hovermode = 'closest',\n",
    "    xaxis = dict(\n",
    "        title='Subgroup AUC'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title='Subgroup Size'\n",
    "    ),\n",
    "    showlegend = True\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace0, trace1, trace2, trace3], layout=layout)\n",
    "py.iplot(fig)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def scale(x):\n",
    "    mini = 272\n",
    "    maxi = 6047\n",
    "    scale_rng = [10, 55]\n",
    "    return (scale_rng[1] - scale_rng[0])*((x-mini)/(maxi-mini))+scale_rng[0]\n",
    "    \n",
    "trace0 = go.Scatter(\n",
    "{\n",
    "        'x': ctrl_bias_metrics['bnsp_auc'], \n",
    "        'y': ctrl_bias_metrics['bpsn_auc'],\n",
    "        'legendgroup': 'w/o pretrained',\n",
    "        'name': 'w/o pretrained', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][0],\n",
    "            'size': [scale(x) for x in ctrl_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': ctrl_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "{\n",
    "        'x': ft_bias_metrics['bnsp_auc'], \n",
    "        'y': ft_bias_metrics['bpsn_auc'],\n",
    "        'legendgroup': 'fasttext',\n",
    "        'name': 'fasttext', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][2],\n",
    "            'size': [scale(x) for x in ft_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': ft_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "{\n",
    "        'x': gl_bias_metrics['bnsp_auc'], \n",
    "        'y': gl_bias_metrics['bpsn_auc'],\n",
    "        'legendgroup': 'glove',\n",
    "        'name': 'glove', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][4],\n",
    "            'size': [scale(x) for x in gl_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': gl_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "{\n",
    "        'x': nb_bias_metrics['bnsp_auc'], \n",
    "        'y': nb_bias_metrics['bpsn_auc'],\n",
    "        'legendgroup': 'numberbatch',\n",
    "        'name': 'numberbatch', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][6],\n",
    "            'size': [scale(x) for x in nb_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': nb_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title= 'Word Embeddings Comparison',\n",
    "    hovermode = 'closest',\n",
    "    xaxis = dict(\n",
    "        title='BNSP-AUC'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title='BPSN-AUC'\n",
    "    ),\n",
    "    showlegend = True\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace0, trace1, trace2, trace3], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate GloVe and Fasttext\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "vec_files = [GLOVE_PATH, FASTTEXT_PATH]\n",
    "glft_matrix = np.concatenate(\n",
    "    [build_matrix(tokenizer.word_index, f) for f in vec_files], axis=-1)\n",
    "\n",
    "# The 600 dimension embedding takes 3h 36min to run compared to \n",
    "# ~3h 3min for the other 300 dimension embedding models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      " - 613s - loss: 0.2240 - dense_3_loss: 0.1337 - dense_4_loss: 0.0903\n",
      "Epoch 1/1\n",
      " - 611s - loss: 0.1999 - dense_3_loss: 0.1163 - dense_4_loss: 0.0836\n",
      "Epoch 1/1\n",
      " - 613s - loss: 0.1928 - dense_3_loss: 0.1108 - dense_4_loss: 0.0821\n",
      "Epoch 1/1\n",
      " - 612s - loss: 0.1877 - dense_3_loss: 0.1066 - dense_4_loss: 0.0811\n",
      "Epoch 1/1\n",
      " - 615s - loss: 0.2239 - dense_7_loss: 0.1335 - dense_8_loss: 0.0903\n",
      "Epoch 1/1\n",
      " - 613s - loss: 0.1997 - dense_7_loss: 0.1162 - dense_8_loss: 0.0836\n",
      "Epoch 1/1\n",
      " - 613s - loss: 0.1927 - dense_7_loss: 0.1107 - dense_8_loss: 0.0820\n",
      "Epoch 1/1\n",
      " - 614s - loss: 0.1878 - dense_7_loss: 0.1067 - dense_8_loss: 0.0810\n"
     ]
    }
   ],
   "source": [
    "glft_preds = train_model(glft_matrix, 'glove+fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# results_df = pd.concat([pd.DataFrame(glft_preds, columns=['glove+fast']).reset_index(drop=True), results_df], axis=1)\n",
    "glft_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'glove+fast', 'target')\n",
    "overall_auc = auc(results_df['target'], results_df['glove+fast'])\n",
    "glft_final_bias = compute_final_bias(glft_bias_metrics, overall_auc)\n",
    "display(glft_bias_metrics)\n",
    "print(f'Final Metric: {glft_final_bias}')\n",
    "del(glft_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighing Results by Bias Scores\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "bias_sum = nb_final_bias + gl_final_bias + ft_final_bias\n",
    "nb_weight = nb_final_bias/bias_sum\n",
    "gl_weight = gl_final_bias/bias_sum\n",
    "ft_weight = ft_final_bias/bias_sum\n",
    "\n",
    "# results_df['weighted'] = results_df.apply(lambda x: (ft_weight*x['fasttext']) + (gl_weight*x['glove']) + (nb_weight*x['numberbatch']), axis=1)\n",
    "w_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'weighted', 'target')\n",
    "overall_auc = auc(results_df['target'], results_df['weighted'])\n",
    "w_final_bias = compute_final_bias(w_bias_metrics, overall_auc)\n",
    "display(w_bias_metrics)\n",
    "print(f'Final Metric: {w_final_bias}')\n",
    "print(f'Glove Weight: {gl_weight}')\n",
    "print(f'Fasttext Weight: {ft_weight}')\n",
    "print(f'Numberbatch Weight: {nb_weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the low score for the Conceptnet Numberbatch embeddings, the weighted average still produces a final bias better than each of the constituent embedding predictions. Later we will train a model with an embedding matrix that is the weighted average of the other embedding matrices, using the same weights we used to combine predictions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Meta Embeddings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "meta_matrix = np.divide(fasttext_matrix + glove_matrix + numb_matrix, 3)\n",
    "# AJ_PREDS\n",
    "meta_preds = train_model(meta_matrix, 'meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# results_df = pd.concat([pd.DataFrame(meta_preds, columns=['meta']).reset_index(drop=True), results_df], axis=1)\n",
    "m_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'meta', 'target')\n",
    "overall_auc = auc(results_df['target'], results_df['meta'])\n",
    "m_final_bias = compute_final_bias(m_bias_metrics, overall_auc)\n",
    "display(m_bias_metrics)\n",
    "print(f'Final Metric: {m_final_bias}')\n",
    "del (meta_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe and Fasttext Meta Embedding\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "meta2_matrix = np.divide(fasttext_matrix + glove_matrix, 2)\n",
    "# AJ_PREDS\n",
    "meta2_preds = train_model(meta2_matrix, 'meta-glove+fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# results_df = pd.concat([pd.DataFrame(meta2_preds, columns=['meta-glove+fasttext']).reset_index(drop=True), results_df], axis=1)\n",
    "m2_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'meta-glove+fasttext', 'target')\n",
    "overall_auc = auc(results_df['target'], results_df['meta-glove+fasttext'])\n",
    "m2_final_bias = compute_final_bias(m2_bias_metrics, overall_auc)\n",
    "display(m2_bias_metrics)\n",
    "print(f'Final Metric: {m2_final_bias}')\n",
    "del (meta2_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Meta Embedding\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "nb_weighted = np.divide(numb_matrix, nb_weight)\n",
    "gl_weighted = np.divide(glove_matrix, gl_weight)\n",
    "ft_weighted = np.divide(fasttext_matrix, ft_weight)\n",
    "meta_weighted_matrix = nb_weighted+gl_weighted+ft_weighted\n",
    "# AJ_PREDS\n",
    "meta_weighted_preds = train_model(meta_weighted_matrix, 'meta-weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# results_df = pd.concat([pd.DataFrame(meta_weighted_preds, columns=['meta-weighted']).reset_index(drop=True), results_df], axis=1)\n",
    "mw_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'meta-weighted', 'target')\n",
    "overall_auc = auc(results_df['target'], results_df['meta-weighted'])\n",
    "mw_final_bias = compute_final_bias(mw_bias_metrics, overall_auc)\n",
    "display(mw_bias_metrics)\n",
    "print(f'Final Metric: {mw_final_bias}')\n",
    "del (meta_weighted_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Embeddings Comparison \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "final_bias = {'w/o pretrained': ctrl_final_bias, 'fasttext': ft_final_bias, 'glove': gl_final_bias, 'numberbatch': nb_final_bias, 'weighted': w_final_bias,'glove+fasttext': glft_final_bias,'meta': m_final_bias, 'meta-glove+fasttext': m2_final_bias, 'meta-weighted': mw_final_bias}\n",
    "final_bias = pd.DataFrame(data = final_bias, index=['final bias score'])\n",
    "display(final_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "trace5 = go.Scatter(\n",
    "{\n",
    "        'x': glft_bias_metrics['bnsp_auc'], \n",
    "        'y': glft_bias_metrics['bpsn_auc'],\n",
    "        'legendgroup': 'glove+fasttext',\n",
    "        'name': 'glove+fasttext', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][1],\n",
    "            'size': [scale(x) for x in glft_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': glft_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "trace6 = go.Scatter(\n",
    "{\n",
    "        'x': m_bias_metrics['bnsp_auc'], \n",
    "        'y': m_bias_metrics['bpsn_auc'],\n",
    "        'legendgroup': 'meta-embedding',\n",
    "        'name': 'meta-embedding', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][3],\n",
    "            'size': [scale(x) for x in m_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': m_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "trace7 = go.Scatter(\n",
    "{\n",
    "        'x': m2_bias_metrics['bnsp_auc'], \n",
    "        'y': m2_bias_metrics['bpsn_auc'],\n",
    "        'legendgroup': 'meta-glove+fasttext',\n",
    "        'name': 'meta-glove+fasttext', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][5],\n",
    "            'size': [scale(x) for x in m2_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': m2_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "trace8 = go.Scatter(\n",
    "{\n",
    "        'x': mw_bias_metrics['bnsp_auc'], \n",
    "        'y': mw_bias_metrics['bpsn_auc'],\n",
    "        'legendgroup': 'meta-weighted',\n",
    "        'name': 'meta-weighted', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][7],\n",
    "            'size': [scale(x) for x in mw_bias_metrics['subgroup_size']]\n",
    "        },\n",
    "        'text': mw_bias_metrics['subgroup']\n",
    "    })\n",
    "\n",
    "layout = go.Layout(\n",
    "    title= 'Concatenated and Meta Word Embeddings Comparison',\n",
    "    hovermode = 'closest',\n",
    "    xaxis = dict(\n",
    "        title='BNSP-AUC'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title='BPSN-AUC'\n",
    "    ),\n",
    "    showlegend = True\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace5, trace6, trace7, trace8], layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "layout = go.Layout(\n",
    "    title= 'Complete Embeddings Comparison',\n",
    "    hovermode = 'closest',\n",
    "    xaxis = dict(\n",
    "        title='BNSP-AUC'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title='BPSN-AUC'\n",
    "    ),\n",
    "    showlegend = True\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace0, trace1, trace2, trace3, trace5, trace6, trace7, trace8], layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "trace0 = go.Scatter(\n",
    "{\n",
    "        'x': final_bias['w/o pretrained'], \n",
    "        'y': [0],\n",
    "        'name': 'w/o pretrained', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][0],\n",
    "            'size': 25\n",
    "        }\n",
    "    })\n",
    "trace1 = go.Scatter(\n",
    "{\n",
    "        'x': final_bias['fasttext'], \n",
    "        'y': [0],\n",
    "        'name': 'fasttext', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][2],\n",
    "            'size': 25\n",
    "        }\n",
    "    })\n",
    "trace2 = go.Scatter(\n",
    "{\n",
    "        'x': final_bias['glove'], \n",
    "        'y': [0],\n",
    "        'name': 'glove', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][4],\n",
    "            'size': 25\n",
    "        }\n",
    "    })\n",
    "trace3 = go.Scatter(\n",
    "{\n",
    "        'x': final_bias['numberbatch'], \n",
    "        'y': [0],\n",
    "        'name': 'numberbatch', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][6],\n",
    "            'size': 25\n",
    "        }\n",
    "    })\n",
    "trace5 = go.Scatter(\n",
    "{\n",
    "        'x': final_bias['glove+fasttext'], \n",
    "        'y': [0],\n",
    "        'name': 'glove+fasttext', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][1],\n",
    "            'size': 25\n",
    "        }\n",
    "    })\n",
    "trace6 = go.Scatter(\n",
    "{\n",
    "        'x': final_bias['meta'], \n",
    "        'y': [0],\n",
    "        'name': 'meta', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][3],\n",
    "            'size': 25\n",
    "        }\n",
    "    })\n",
    "trace7 = go.Scatter(\n",
    "{\n",
    "        'x': final_bias['meta-glove+fasttext'], \n",
    "        'y': [0],\n",
    "        'name': 'meta-glove+fasttext', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][5],\n",
    "            'size': 25\n",
    "        }\n",
    "    })\n",
    "trace8 = go.Scatter(\n",
    "{\n",
    "        'x': final_bias['meta-weighted'], \n",
    "        'y': [0],\n",
    "        'name': 'meta-weighted', \n",
    "        'mode': 'markers', \n",
    "        'marker': {\n",
    "            'color': cl.scales['9']['div']['Spectral'][7],\n",
    "            'size': 25\n",
    "        }\n",
    "    })\n",
    "\n",
    "layout = go.Layout(\n",
    "    title= 'Final Bias Score for All Embeddings',\n",
    "    hovermode = 'closest',\n",
    "    xaxis = dict(\n",
    "        title='Final Bias Score'\n",
    "    ),\n",
    "    showlegend = True\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace0, trace1, trace2, trace3, trace5, trace6, trace7, trace8], layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
